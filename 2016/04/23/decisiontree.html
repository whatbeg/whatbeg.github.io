
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>如何实现并应用决策树算法？ | Whatbeg&#39;s blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="whatbeg">
    

    
    <meta name="description" content="本文对决策树算法进行简单的总结和梳理，并对著名的决策树算法ID3(Iterative Dichotomiser 迭代二分器)进行实现，实现采用Python语言，一句老梗，“人生苦短，我用Python”，Python确实能够省很多语言方面的事，从而可以让我们专注于问题和解决问题的逻辑。根据不同的数据，我实现了三个版本的ID3算法，复杂度逐步提升：1.纯标称值无缺失数据集2.连续值和标称值混合且无缺失">
<meta property="og:type" content="article">
<meta property="og:title" content="如何实现并应用决策树算法？">
<meta property="og:url" content="http://whatbeg.com/2016/04/23/decisiontree.html">
<meta property="og:site_name" content="Whatbeg's blog">
<meta property="og:description" content="本文对决策树算法进行简单的总结和梳理，并对著名的决策树算法ID3(Iterative Dichotomiser 迭代二分器)进行实现，实现采用Python语言，一句老梗，“人生苦短，我用Python”，Python确实能够省很多语言方面的事，从而可以让我们专注于问题和解决问题的逻辑。根据不同的数据，我实现了三个版本的ID3算法，复杂度逐步提升：1.纯标称值无缺失数据集2.连续值和标称值混合且无缺失">
<meta property="og:image" content="http://7xsl28.com2.z0.glb.clouddn.com/dectree2.png">
<meta property="og:image" content="http://7xsl28.com2.z0.glb.clouddn.com/ID3algorithm.png-SuoLve.Shuiyin">
<meta property="og:image" content="http://7xsl28.com2.z0.glb.clouddn.com/dectree1.png-SuoLve.Shuiyin">
<meta property="og:image" content="http://7xsl28.com2.z0.glb.clouddn.com/dectree2.png-SuoLve.Shuiyin">
<meta property="og:image" content="http://7xsl28.com2.z0.glb.clouddn.com/dectreegongshi.png-SuoLve.Shuiyin">
<meta property="og:image" content="http://7xsl28.com2.z0.glb.clouddn.com/dectree3.png">
<meta property="og:image" content="http://7xsl28.com2.z0.glb.clouddn.com/dectree4.png">
<meta property="og:image" content="http://7xsl28.com2.z0.glb.clouddn.com/canceroutput.png">
<meta property="og:updated_time" content="2016-05-12T03:16:30.061Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="如何实现并应用决策树算法？">
<meta name="twitter:description" content="本文对决策树算法进行简单的总结和梳理，并对著名的决策树算法ID3(Iterative Dichotomiser 迭代二分器)进行实现，实现采用Python语言，一句老梗，“人生苦短，我用Python”，Python确实能够省很多语言方面的事，从而可以让我们专注于问题和解决问题的逻辑。根据不同的数据，我实现了三个版本的ID3算法，复杂度逐步提升：1.纯标称值无缺失数据集2.连续值和标称值混合且无缺失">
<meta name="twitter:image" content="http://7xsl28.com2.z0.glb.clouddn.com/dectree2.png">

    
    <link rel="alternative" href="/atom.xml" title="Whatbeg&#39;s blog" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/w.ico">
    
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Whatbeg&#39;s blog">Whatbeg&#39;s blog</a></h1>
				<h2 class="blog-motto">当你的才华撑不起你的野心时，就应该静下心来好好学习。</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">首页(Home)</a></li>
					
						<li><a href="/archives">归档(Archives)</a></li>
					
						<li><a href="/tags">标签(Tags)</a></li>
					
						<li><a href="/categories">分类(Categories)</a></li>
					
						<li><a href="/about">关于(About)</a></li>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/04/23/decisiontree.html" title="如何实现并应用决策树算法？" itemprop="url">如何实现并应用决策树算法？</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="whatbeg" target="_blank" itemprop="author">whatbeg</a>
		
  <p class="article-time">
    <time datetime="2016-04-23T12:35:01.000Z" itemprop="datePublished"> 发表于 2016-04-23</time>
    <span id="busuanzi_container_page_pv">
    总阅读<span id="busuanzi_value_page_pv"></span>次
    </span>
  </p>

</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树简介"><span class="toc-number">1.</span> <span class="toc-text">决策树简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ID3算法简介"><span class="toc-number">2.</span> <span class="toc-text">ID3算法简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ID3算法实现（纯标称值）"><span class="toc-number">3.</span> <span class="toc-text">ID3算法实现（纯标称值）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#有连续值的情况"><span class="toc-number">4.</span> <span class="toc-text">有连续值的情况</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#有缺失值的情况"><span class="toc-number">5.</span> <span class="toc-text">有缺失值的情况</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#在乳腺癌数据集上的测试与表现"><span class="toc-number">6.</span> <span class="toc-text">在乳腺癌数据集上的测试与表现</span></a></li></ol>
		
		</div>
		
		<p>本文对决策树算法进行简单的总结和梳理，并对著名的决策树算法ID3(Iterative Dichotomiser 迭代二分器)进行实现，实现采用Python语言，一句老梗，“人生苦短，我用Python”，Python确实能够省很多语言方面的事，从而可以让我们专注于问题和解决问题的逻辑。<br>根据不同的数据，我实现了三个版本的ID3算法，复杂度逐步提升：<br>1.纯标称值无缺失数据集<br>2.连续值和标称值混合且无缺失数据集<br>3.连续值和标称值混合，有缺失数据集<br>第一个算法参考了《机器学习实战》的大部分代码，第二、三个算法基于前面的实现进行模块的增加。</p>
<h2 id="决策树简介"><a href="#决策树简介" class="headerlink" title="决策树简介"></a>决策树简介</h2><p>决策树算法不用说大家应该都知道，是机器学习的一个著名算法，由澳大利亚著名计算机科学家Rose Quinlan发表。</p>
<p>决策树是一种监督学习的分类算法，目的是学习出一颗决策树，该树中间节点是数据特征，叶子节点是类别，实际分类时根据树的结构，一步一步根据当前数据特征取值选择进入哪一颗子树，直到走到叶子节点，叶子节点的类别就是此决策树对此数据的学习结果。下图就是一颗简单的决策树：<br><img src="http://7xsl28.com2.z0.glb.clouddn.com/dectree2.png" alt="一颗简单的决策树"><br>此决策树用来判断一个具有纹理，触感，密度的西瓜是否是“好瓜”。<br>当有这样一个西瓜，纹理清晰，密度为0.333，触感硬滑，那么要你判断是否是一个“好瓜”，这时如果通过决策树来判断，显然可以一直顺着纹理-&gt;清晰-&gt;密度&lt;=0.382-&gt;否，即此瓜不是“好瓜”，一次决策就这样完成了。正因为决策树决策很方便，并且准确率也较高，所以常常被用来做分类器，也是“机器学习十大算法”之一C4.5的基本思想。<br>学习出一颗决策树首要考虑一个问题，即 根据数据集构建当前树应该选择哪种属性作为树根，即划分标准？<br>考虑最好的情况，一开始选择某个特征，就把数据集划分成功，即在该特征上取某个值的全是一类。<br>考虑最坏的情况，不断选择特征，划分后的数据集总是杂乱无章，就二分类任务来说，总是有正类有负类，一直到特征全部用完了，划分的数据集合还是有正有负，这时只能用投票法，正类多就选正类作为叶子，否则选负类。<br>所以得出了一般结论： 随着划分的进行，我们希望选择一个特征，使得子节点包含的样本尽可能属于同一类别，即“纯度”越高越好。</p>
<p>基于“纯度”的标准不同，有三种算法：<br>1.ID3算法(Iterative Dichotomiser 迭代二分器)，也是本文要实现的算法，基于信息增益即信息熵来度量纯度<br>2.C4.5算法(Classifier 4.5)，ID3 的后继算法，也是昆兰提出<br>3.CART算法(Classification And Regression Tree)，基于基尼指数度量纯度。</p>
<h2 id="ID3算法简介"><a href="#ID3算法简介" class="headerlink" title="ID3算法简介"></a>ID3算法简介</h2><p>信息熵是信息论中的一个重要概念，也叫“香农熵”，香农先生的事迹相比很多人都听过，一个人开创了一门理论，牛的不行。香农理论中一个很重要的特征就是”熵“，即”信息内容的不确定性“，香农在进行信息的定量计算的时候，明确地把信息量定义为随机不定性程度的减少。这就表明了他对信息的理解：信息是用来减少随机不定性的东西。或者表达为香农逆定义：信息是确定性的增加。这也印证了决策树以熵作为划分选择的度量标准的正确性，即我们想更快速地从数据中获得更多信息，我们就应该快速降低不确定性，即减少”熵“。</p>
<p>信息熵定义为：<br>$$Ent(D) = -\sum_{k=1}^{|\mathcal{Y}|}p_k\log_2p_k$$<br>D表示数据集，类别总数为|Y|，pk表示D中第k类样本所占的比例。根据其定义，Ent的值越小，信息纯度越高。Ent的范围是[0,log|Y|]</p>
<p>下面要选择某个属性进行划分，要依次考虑每个属性，假设当前考虑属性a，a的取值有|V|种，那么我们希望取a作为划分属性，划分到|V|个子节点后，所有子节点的信息熵之和即划分后的信息熵能够有很大的减小，减小的最多的那个属性就是我们选择的属性。</p>
<p>划分后的信息熵定义为：<br>$$\sum_{v=1}^V{|D^v| \over |D|}Ent(D^v) $$</p>
<p>所以用属性a对样本集D进行划分的信息增益就是原来的信息熵减去划分后的信息熵：<br>$$Gain(D,a) = Ent(D) - \sum_{v=1}^V{|D^v| \over |D|}Ent(D^v) $$</p>
<p>ID3算法就是这样每次选择一个属性对样本集进行划分，知道两种情况使这个过程停止：<br>（1）某个子节点样本全部属于一类<br>（2）属性都用完了，这时候如果子节点样本还是不一致，那么只好少数服从多数了<br>算法流程如下：(图片来自网络)<br><img src="http://7xsl28.com2.z0.glb.clouddn.com/ID3algorithm.png-SuoLve.Shuiyin" alt="ID3算法"></p>
<h2 id="ID3算法实现（纯标称值）"><a href="#ID3算法实现（纯标称值）" class="headerlink" title="ID3算法实现（纯标称值）"></a>ID3算法实现（纯标称值）</h2><p>如果样本全部是标称值即离散值的话，会比较简单。<br>代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#决策树生成算法： treesID3.py</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span>            <span class="comment">#创建数据集</span></span><br><span class="line">    dataSet = [[<span class="number">1</span>,<span class="number">1</span>,<span class="string">'yes'</span>],</span><br><span class="line">               [<span class="number">1</span>,<span class="number">1</span>,<span class="string">'yes'</span>],</span><br><span class="line">               [<span class="number">1</span>,<span class="number">0</span>,<span class="string">'no'</span>],</span><br><span class="line">               [<span class="number">0</span>,<span class="number">1</span>,<span class="string">'no'</span>],</span><br><span class="line">               [<span class="number">0</span>,<span class="number">1</span>,<span class="string">'no'</span>]]</span><br><span class="line">    featname = [<span class="string">'no surfacing'</span>, <span class="string">'flippers'</span>]</span><br><span class="line">    <span class="keyword">return</span> dataSet,featname</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filetoDataSet</span><span class="params">(filename)</span>:</span></span><br><span class="line">    fr = open(filename,<span class="string">'r'</span>)</span><br><span class="line">    all_lines = fr.readlines()</span><br><span class="line">    featname = all_lines[<span class="number">0</span>].strip().split(<span class="string">','</span>)[<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">    print(featname)</span><br><span class="line">    dataSet = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> all_lines[<span class="number">1</span>:]:</span><br><span class="line">        line = line.strip()</span><br><span class="line">        lis = line.split(<span class="string">','</span>)[<span class="number">1</span>:]</span><br><span class="line">        dataSet.append(lis)</span><br><span class="line">    fr.close()</span><br><span class="line">    <span class="keyword">return</span> dataSet,featname</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcEnt</span><span class="params">(dataSet)</span>:</span>           <span class="comment">#计算香农熵</span></span><br><span class="line">    numEntries = len(dataSet)</span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        label = featVec[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[label] = <span class="number">0</span></span><br><span class="line">        labelCounts[label] += <span class="number">1</span></span><br><span class="line">    Ent = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">        p_i = float(labelCounts[key]/numEntries)</span><br><span class="line">        Ent -= p_i * log(p_i,<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> Ent</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, axis, value)</span>:</span>   <span class="comment">#划分数据集,找出第axis个属性为value的数据</span></span><br><span class="line">    returnSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:</span><br><span class="line">            retVec = featVec[:axis]</span><br><span class="line">            retVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            returnSet.append(retVec)</span><br><span class="line">    <span class="keyword">return</span> returnSet</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeat</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numFeat = len(dataSet[<span class="number">0</span>])<span class="number">-1</span></span><br><span class="line">    Entropy = calcEnt(dataSet)</span><br><span class="line">    DataSetlen = float(len(dataSet))</span><br><span class="line">    bestGain = <span class="number">0.0</span></span><br><span class="line">    bestFeat = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeat):</span><br><span class="line">        allvalue = [featVec[i] <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet]</span><br><span class="line">        specvalue = set(allvalue)</span><br><span class="line">        nowEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> specvalue:</span><br><span class="line">            Dv = splitDataSet(dataSet,i,v)</span><br><span class="line">            p = len(Dv)/DataSetlen</span><br><span class="line">            nowEntropy += p * calcEnt(Dv)</span><br><span class="line">        <span class="keyword">if</span> Entropy - nowEntropy &gt; bestGain:</span><br><span class="line">            bestGain = Entropy - nowEntropy</span><br><span class="line">            bestFeat = i</span><br><span class="line">    <span class="keyword">return</span> bestFeat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Vote</span><span class="params">(classList)</span>:</span></span><br><span class="line">    classdic = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classdic.keys():</span><br><span class="line">            classdic[vote] = <span class="number">0</span></span><br><span class="line">        classdic[vote] += <span class="number">1</span></span><br><span class="line">    sortedclassDic = sorted(classdic.items(),key=itemgetter(<span class="number">1</span>),reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedclassDic[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDecisionTree</span><span class="params">(dataSet,featnames)</span>:</span></span><br><span class="line">    featname = featnames[:]              <span class="comment">################</span></span><br><span class="line">    classlist = [featvec[<span class="number">-1</span>] <span class="keyword">for</span> featvec <span class="keyword">in</span> dataSet]  <span class="comment">#此节点的分类情况</span></span><br><span class="line">    <span class="keyword">if</span> classlist.count(classlist[<span class="number">0</span>]) == len(classlist):  <span class="comment">#全部属于一类</span></span><br><span class="line">        <span class="keyword">return</span> classlist[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:         <span class="comment">#分完了,没有属性了</span></span><br><span class="line">        <span class="keyword">return</span> Vote(classlist)       <span class="comment">#少数服从多数</span></span><br><span class="line">    <span class="comment"># 选择一个最优特征进行划分</span></span><br><span class="line">    bestFeat = chooseBestFeat(dataSet)</span><br><span class="line">    bestFeatname = featname[bestFeat]</span><br><span class="line">    <span class="keyword">del</span>(featname[bestFeat])     <span class="comment">#防止下标不准</span></span><br><span class="line">    DecisionTree = &#123;bestFeatname:&#123;&#125;&#125;</span><br><span class="line">    <span class="comment"># 创建分支,先找出所有属性值,即分支数</span></span><br><span class="line">    allvalue = [vec[bestFeat] <span class="keyword">for</span> vec <span class="keyword">in</span> dataSet]</span><br><span class="line">    specvalue = sorted(list(set(allvalue)))  <span class="comment">#使有一定顺序</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> specvalue:</span><br><span class="line">        copyfeatname = featname[:]</span><br><span class="line">        DecisionTree[bestFeatname][v] = createDecisionTree(splitDataSet(dataSet,bestFeat,v),copyfeatname)</span><br><span class="line">    <span class="keyword">return</span> DecisionTree</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    filename = <span class="string">"D:\\MLinAction\\Data\\西瓜2.0.txt"</span></span><br><span class="line">    DataSet,featname = filetoDataSet(filename)</span><br><span class="line">    Tree = createDecisionTree(DataSet,featname)</span><br><span class="line">    print(Tree)</span><br></pre></td></tr></table></figure></p>
<p>解释一下几个函数：</p>
<blockquote>
<p>filetoDataSet(filename)  将文件中的数据整理成数据集<br>calcEnt(dataSet)     计算香农熵<br>splitDataSet(dataSet, axis, value)     划分数据集，选择出第axis个属性的取值为value的所有数据集，即D^v，并去掉第axis个属性，因为不需要了<br>chooseBestFeat(dataSet)      根据信息增益，选择一个最好的属性<br>Vote(classList)        如果属性用完，类别仍不一致，投票决定<br>createDecisionTree(dataSet,featnames)     递归创建决策树</p>
</blockquote>
<p>用西瓜数据集2.0对算法进行测试，西瓜数据集见 <a href="http://whatbeg.com/2016/04/22/xiguadataset.html">西瓜数据集2.0</a>，输出如下：<br><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="symbol">'色泽</span>', <span class="symbol">'根蒂</span>', <span class="symbol">'敲声</span>', <span class="symbol">'纹理</span>', <span class="symbol">'脐部</span>', <span class="symbol">'触感</span>']</span><br><span class="line">&#123;<span class="symbol">'纹理</span><span class="symbol">':</span> &#123;<span class="symbol">'清晰</span><span class="symbol">':</span> &#123;<span class="symbol">'根蒂</span><span class="symbol">':</span> &#123;<span class="symbol">'蜷缩</span><span class="symbol">':</span> <span class="symbol">'是</span>', <span class="symbol">'硬挺</span><span class="symbol">':</span> <span class="symbol">'否</span>', <span class="symbol">'稍蜷</span><span class="symbol">':</span> &#123;<span class="symbol">'色泽</span><span class="symbol">':</span> &#123;<span class="symbol">'青绿</span><span class="symbol">':</span> <span class="symbol">'是</span>', <span class="symbol">'乌黑</span><span class="symbol">':</span> &#123;<span class="symbol">'触感</span><span class="symbol">':</span> &#123;<span class="symbol">'硬滑</span><span class="symbol">':</span> <span class="symbol">'是</span>', <span class="symbol">'软粘</span><span class="symbol">':</span> <span class="symbol">'否</span>'&#125;&#125;&#125;&#125;&#125;&#125;, <span class="symbol">'稍糊</span><span class="symbol">':</span> &#123;<span class="symbol">'触感</span><span class="symbol">':</span> &#123;<span class="symbol">'硬滑</span><span class="symbol">':</span> <span class="symbol">'否</span>', <span class="symbol">'软粘</span><span class="symbol">':</span> <span class="symbol">'是</span>'&#125;&#125;, <span class="symbol">'模糊</span><span class="symbol">':</span> <span class="symbol">'否</span>'&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<p><img src="http://7xsl28.com2.z0.glb.clouddn.com/dectree1.png-SuoLve.Shuiyin" alt="决策树算法在西瓜数据集2.0上的结果"><br>由于数据太少，没有设置测试数据以验证其准确度，但是我后面会根据乳腺癌的例子进行准确度的测试的，下面进入下一部分：</p>
<h2 id="有连续值的情况"><a href="#有连续值的情况" class="headerlink" title="有连续值的情况"></a>有连续值的情况</h2><p>有连续值的情况如 <a href="http://whatbeg.com/2016/04/22/xiguadataset.html">西瓜数据集3.0</a><br>一个属性有很多种取值，我们肯定不能每个取值都做一个分支，这时候需要对连续属性进行离散化，有几种方法供选择，其中两种是：<br>1.对每一类别的数据集的连续值取平均值，再取各类的平均值的平均值作为划分点，将连续属性化为两类变成离散属性<br>2.C4.5采用的二分法，排序离散属性，取每两个的中点作为划分点的候选点，计算以每个划分点划分数据集的信息增益，取最大的那个划分点将连续属性化为两类变成离散属性，用该属性进行划分的信息增益就是刚刚计算的最大信息增益。公式如下：<br>$$Gain(D,a) = \max_{t \in T_a}Ent(D) - \sum_{\lambda \in {+,-}}{|D_t^\lambda| \over |D|}Ent(D_t^\lambda) $$<br>这里采用第二种，并在学习前对连续属性进行离散化。增加处理的代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet_for_dec</span><span class="params">(dataSet, axis, value, small)</span>:</span></span><br><span class="line">    returnSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> (small <span class="keyword">and</span> featVec[axis] &lt;= value) <span class="keyword">or</span> ((<span class="keyword">not</span> small) <span class="keyword">and</span> featVec[axis] &gt; value):</span><br><span class="line">            retVec = featVec[:axis]</span><br><span class="line">            retVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            returnSet.append(retVec)</span><br><span class="line">    <span class="keyword">return</span> returnSet</span><br><span class="line">            </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DataSetPredo</span><span class="params">(filename,decreteindex)</span>:</span></span><br><span class="line">    dataSet,featname = filetoDataSet(filename)</span><br><span class="line">    Entropy = calcEnt(dataSet)</span><br><span class="line">    DataSetlen = len(dataSet)</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> decreteindex:     <span class="comment">#对每一个是连续值的属性下标</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(DataSetlen):</span><br><span class="line">            dataSet[i][index] = float(dataSet[i][index])</span><br><span class="line">        allvalue = [vec[index] <span class="keyword">for</span> vec <span class="keyword">in</span> dataSet]</span><br><span class="line">        sortedallvalue = sorted(allvalue)</span><br><span class="line">        T = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(allvalue)<span class="number">-1</span>):        <span class="comment">#划分点集合</span></span><br><span class="line">            T.append(float(sortedallvalue[i]+sortedallvalue[i+<span class="number">1</span>])/<span class="number">2.0</span>)</span><br><span class="line">        bestGain = <span class="number">0.0</span></span><br><span class="line">        bestpt = <span class="number">-1.0</span></span><br><span class="line">        <span class="keyword">for</span> pt <span class="keyword">in</span> T:          <span class="comment">#对每个划分点</span></span><br><span class="line">            nowent = <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">for</span> small <span class="keyword">in</span> range(<span class="number">2</span>):   <span class="comment">#化为正类负类</span></span><br><span class="line">                Dt = splitDataSet_for_dec(dataSet, index, pt, small)</span><br><span class="line">                p = len(Dt) / float(DataSetlen)</span><br><span class="line">                nowent += p * calcEnt(Dt)</span><br><span class="line">            <span class="keyword">if</span> Entropy - nowent &gt; bestGain:</span><br><span class="line">                bestGain = Entropy-nowent</span><br><span class="line">                bestpt = pt</span><br><span class="line">        featname[index] = str(featname[index]+<span class="string">"&lt;="</span>+<span class="string">"%.3f"</span>%bestpt)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(DataSetlen):</span><br><span class="line">            dataSet[i][index] = <span class="string">"是"</span> <span class="keyword">if</span> dataSet[i][index] &lt;= bestpt <span class="keyword">else</span> <span class="string">"否"</span></span><br><span class="line">    <span class="keyword">return</span> dataSet,featname</span><br></pre></td></tr></table></figure></p>
<p>输出处理过后的属性以及对西瓜数据集3.0的输出<br><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="symbol">'色泽</span>', <span class="symbol">'根蒂</span>', <span class="symbol">'敲声</span>', <span class="symbol">'纹理</span>', <span class="symbol">'脐部</span>', <span class="symbol">'触感</span>', <span class="symbol">'密度&lt;=0.382</span>', <span class="symbol">'含糖率&lt;=0.126</span>']</span><br><span class="line">&#123;<span class="symbol">'纹理</span><span class="symbol">':</span> &#123;<span class="symbol">'模糊</span><span class="symbol">':</span> <span class="symbol">'否</span>', <span class="symbol">'稍糊</span><span class="symbol">':</span> &#123;<span class="symbol">'触感</span><span class="symbol">':</span> &#123;<span class="symbol">'硬滑</span><span class="symbol">':</span> <span class="symbol">'否</span>', <span class="symbol">'软粘</span><span class="symbol">':</span> <span class="symbol">'是</span>'&#125;&#125;, <span class="symbol">'清晰</span><span class="symbol">':</span> &#123;<span class="symbol">'密度&lt;=0.382</span><span class="symbol">':</span> &#123;<span class="symbol">'是</span><span class="symbol">':</span> <span class="symbol">'否</span>', <span class="symbol">'否</span><span class="symbol">':</span> <span class="symbol">'是</span>'&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<p>主要是预处理函数DataSetPredo，对数据集提前离散化，然后再进行学习，学习代码类似。输出的决策树如下：<br><img src="http://7xsl28.com2.z0.glb.clouddn.com/dectree2.png-SuoLve.Shuiyin" alt="决策树算法在西瓜数据集3.0上的结果"></p>
<h2 id="有缺失值的情况"><a href="#有缺失值的情况" class="headerlink" title="有缺失值的情况"></a>有缺失值的情况</h2><p>数据有缺失值是常见的情况，我们不好直接抛弃这些数据，因为这样会损失大量数据，不划算，但是缺失值我们也无法判断它的取值。怎么办呢，办法还是有的。<br>考虑两个问题：<br><strong>1.有缺失值时如何进行划分选择</strong><br><strong>2.已选择划分属性，有缺失值的样本划不划分，如何划分？</strong></p>
<p>基本思想是进行最优属性选择时，先只考虑无缺失值样本，然后再乘以相应比例，得到在整个样本集上的大致情况。连带考虑到第二个问题的话，考虑给每一个样本一个权重，此时每个样本不再总是被看成一个独立样本，这样有利于第二个问题的解决：即若样本在属性a上的值缺失，那么将其看成是所有值都取，只不过取每个值的权重不一样，每个值的权重参考该值在无缺失值样本中的比例，简单地说，比如在无缺失值样本集中，属性a取去两个值1和2，并且取1的权重和占整个权重和1/3，而取2的权重和占2/3，那么依据该属性对样本集进行划分时，遇到该属性上有缺失值的样本，那么我们认为该样本取值2的可能性更大，于是将该样本的权重乘以2/3归到取值为2的样本集中继续进行划分构造决策树，而乘1/3划到取值为1的样本集中继续构造。不知道我说清楚没有。</p>
<p>公式如下：<br><img src="http://7xsl28.com2.z0.glb.clouddn.com/dectreegongshi.png-SuoLve.Shuiyin" alt=""></p>
<p>其中，D~表示数据集D在属性a上无缺失值的样本，根据它来判断a属性的优劣，rho(即‘lou’)表示属性a的无缺失值样本占所有样本的比例，p~_k表示无缺失值样本中第k类所占的比例，r~_v表示无缺失值样本在属性a上取值为v的样本所占的比例。<br>在划分样本时，如果有缺失值，则将样本划分到所有子节点，在属性a取值v的子节点上的权重为r~_v * 原来的权重。<br>更详细的解读参考《机器学习》P86-87。<br>根据权重法修改后的ID3算法实现如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#修改 treesID3.py</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filetoDataSet</span><span class="params">(filename)</span>:</span></span><br><span class="line">    fr = open(filename,<span class="string">'r'</span>)</span><br><span class="line">    all_lines = fr.readlines()</span><br><span class="line">    featname = all_lines[<span class="number">0</span>].strip().split(<span class="string">','</span>)[<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">    dataSet = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> all_lines[<span class="number">1</span>:]:</span><br><span class="line">        line = line.strip()</span><br><span class="line">        lis = line.split(<span class="string">','</span>)[<span class="number">1</span>:]</span><br><span class="line">        dataSet.append(lis)</span><br><span class="line">    fr.close()</span><br><span class="line">    <span class="keyword">return</span> dataSet,featname</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcEnt</span><span class="params">(dataSet, weight)</span>:</span>           <span class="comment">#计算权重香农熵</span></span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        label = featVec[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[label] = <span class="number">0</span></span><br><span class="line">        labelCounts[label] += weight[i]</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    Ent = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">        p_i = float(labelCounts[key]/sum(weight))</span><br><span class="line">        Ent -= p_i * log(p_i,<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> Ent</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, weight, axis, value, countmissvalue)</span>:</span>   <span class="comment">#划分数据集,找出第axis个属性为value的数据</span></span><br><span class="line">    returnSet = []</span><br><span class="line">    returnweight = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == <span class="string">'?'</span> <span class="keyword">and</span> (<span class="keyword">not</span> countmissvalue):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> countmissvalue <span class="keyword">and</span> featVec[axis] == <span class="string">'?'</span>:</span><br><span class="line">            retVec = featVec[:axis]</span><br><span class="line">            retVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            returnSet.append(retVec)</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:</span><br><span class="line">            retVec = featVec[:axis]</span><br><span class="line">            retVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            returnSet.append(retVec)</span><br><span class="line">            returnweight.append(weight[i])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnSet,returnweight</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet_for_dec</span><span class="params">(dataSet, axis, value, small, countmissvalue)</span>:</span></span><br><span class="line">    returnSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == <span class="string">'?'</span> <span class="keyword">and</span> (<span class="keyword">not</span> countmissvalue):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> countmissvalue <span class="keyword">and</span> featVec[axis] == <span class="string">'?'</span>:</span><br><span class="line">            retVec = featVec[:axis]</span><br><span class="line">            retVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            returnSet.append(retVec)</span><br><span class="line">        <span class="keyword">if</span> (small <span class="keyword">and</span> featVec[axis] &lt;= value) <span class="keyword">or</span> ((<span class="keyword">not</span> small) <span class="keyword">and</span> featVec[axis] &gt; value):</span><br><span class="line">            retVec = featVec[:axis]</span><br><span class="line">            retVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            returnSet.append(retVec)</span><br><span class="line">    <span class="keyword">return</span> returnSet</span><br><span class="line">            </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DataSetPredo</span><span class="params">(filename,decreteindex)</span>:</span>     <span class="comment">#首先运行，权重不变为1</span></span><br><span class="line">    dataSet,featname = filetoDataSet(filename)</span><br><span class="line">    DataSetlen = len(dataSet)</span><br><span class="line">    Entropy = calcEnt(dataSet,[<span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(DataSetlen)])</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> decreteindex:     <span class="comment">#对每一个是连续值的属性下标</span></span><br><span class="line">        UnmissDatalen = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(DataSetlen):      <span class="comment">#字符串转浮点数</span></span><br><span class="line">            <span class="keyword">if</span> dataSet[i][index] != <span class="string">'?'</span>:</span><br><span class="line">                UnmissDatalen += <span class="number">1</span></span><br><span class="line">                dataSet[i][index] = float(dataSet[i][index])</span><br><span class="line">        allvalue = [vec[index] <span class="keyword">for</span> vec <span class="keyword">in</span> dataSet <span class="keyword">if</span> vec[index] != <span class="string">'?'</span>]</span><br><span class="line">        sortedallvalue = sorted(allvalue)</span><br><span class="line">        T = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(allvalue)<span class="number">-1</span>):        <span class="comment">#划分点集合</span></span><br><span class="line">            T.append(float(sortedallvalue[i]+sortedallvalue[i+<span class="number">1</span>])/<span class="number">2.0</span>)</span><br><span class="line">        bestGain = <span class="number">0.0</span></span><br><span class="line">        bestpt = <span class="number">-1.0</span></span><br><span class="line">        <span class="keyword">for</span> pt <span class="keyword">in</span> T:          <span class="comment">#对每个划分点</span></span><br><span class="line">            nowent = <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">for</span> small <span class="keyword">in</span> range(<span class="number">2</span>):   <span class="comment">#化为正类(1)负类(0)</span></span><br><span class="line">                Dt = splitDataSet_for_dec(dataSet, index, pt, small, <span class="keyword">False</span>)</span><br><span class="line">                p = len(Dt) / float(UnmissDatalen)</span><br><span class="line">                nowent += p * calcEnt(Dt)</span><br><span class="line">            <span class="keyword">if</span> Entropy - nowent &gt; bestGain:</span><br><span class="line">                bestGain = Entropy-nowent</span><br><span class="line">                bestpt = pt</span><br><span class="line">        featname[index] = str(featname[index]+<span class="string">"&lt;="</span>+<span class="string">"%.3f"</span>%bestpt)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(DataSetlen):</span><br><span class="line">            <span class="keyword">if</span> dataSet[i][index] != <span class="string">'?'</span>:</span><br><span class="line">                dataSet[i][index] = <span class="string">"是"</span> <span class="keyword">if</span> dataSet[i][index] &lt;= bestpt <span class="keyword">else</span> <span class="string">"否"</span></span><br><span class="line">    <span class="keyword">return</span> dataSet,featname</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getUnmissDataSet</span><span class="params">(dataSet, weight, axis)</span>:</span></span><br><span class="line">    returnSet = []</span><br><span class="line">    returnweight = []</span><br><span class="line">    tag = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == <span class="string">'?'</span>:</span><br><span class="line">            tag.append(i)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            retVec = featVec[:axis]</span><br><span class="line">            retVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            returnSet.append(retVec)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(weight)):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> tag:</span><br><span class="line">            returnweight.append(weight[i])</span><br><span class="line">    <span class="keyword">return</span> returnSet,returnweight</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printlis</span><span class="params">(lis)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> li <span class="keyword">in</span> lis:</span><br><span class="line">        print(li)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeat</span><span class="params">(dataSet,weight,featname)</span>:</span></span><br><span class="line">    numFeat = len(dataSet[<span class="number">0</span>])<span class="number">-1</span></span><br><span class="line">    DataSetWeight = sum(weight)</span><br><span class="line">    bestGain = <span class="number">0.0</span></span><br><span class="line">    bestFeat = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeat):</span><br><span class="line">        UnmissDataSet,Unmissweight = getUnmissDataSet(dataSet, weight, i)   <span class="comment">#无缺失值数据集及其权重</span></span><br><span class="line">        Entropy = calcEnt(UnmissDataSet,Unmissweight)      <span class="comment">#Ent(D~)</span></span><br><span class="line">        allvalue = [featVec[i] <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet <span class="keyword">if</span> featVec[i] != <span class="string">'?'</span>]</span><br><span class="line">        UnmissSumWeight = sum(Unmissweight)</span><br><span class="line">        lou = UnmissSumWeight / DataSetWeight        <span class="comment">#lou</span></span><br><span class="line">        specvalue = set(allvalue)</span><br><span class="line">        nowEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> specvalue:      <span class="comment">#该属性的几种取值</span></span><br><span class="line">            Dv,weightVec_v = splitDataSet(dataSet,Unmissweight,i,v,<span class="keyword">False</span>)   <span class="comment">#返回 此属性为v的所有样本 以及 每个样本的权重</span></span><br><span class="line">            p = sum(weightVec_v) / UnmissSumWeight          <span class="comment">#r~_v = D~_v / D~</span></span><br><span class="line">            nowEntropy += p * calcEnt(Dv,weightVec_v)</span><br><span class="line">        <span class="keyword">if</span> lou*(Entropy - nowEntropy) &gt; bestGain:</span><br><span class="line">            bestGain = Entropy - nowEntropy</span><br><span class="line">            bestFeat = i</span><br><span class="line">    <span class="keyword">return</span> bestFeat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Vote</span><span class="params">(classList,weight)</span>:</span></span><br><span class="line">    classdic = &#123;&#125;</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classdic.keys():</span><br><span class="line">            classdic[vote] = <span class="number">0</span></span><br><span class="line">        classdic[vote] += weight[i]</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    sortedclassDic = sorted(classdic.items(),key=itemgetter(<span class="number">1</span>),reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedclassDic[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet_adjustWeight</span><span class="params">(dataSet,weight,axis,value,r_v)</span>:</span></span><br><span class="line">    returnSet = []</span><br><span class="line">    returnweight = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == <span class="string">'?'</span>:</span><br><span class="line">            retVec = featVec[:axis]</span><br><span class="line">            retVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            returnSet.append(retVec)</span><br><span class="line">            returnweight.append(weight[i] * r_v)</span><br><span class="line">        <span class="keyword">elif</span> featVec[axis] == value:</span><br><span class="line">            retVec = featVec[:axis]</span><br><span class="line">            retVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            returnSet.append(retVec)</span><br><span class="line">            returnweight.append(weight[i])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnSet,returnweight</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDecisionTree</span><span class="params">(dataSet,weight,featnames)</span>:</span></span><br><span class="line">    featname = featnames[:]              <span class="comment">################</span></span><br><span class="line">    classlist = [featvec[<span class="number">-1</span>] <span class="keyword">for</span> featvec <span class="keyword">in</span> dataSet]  <span class="comment">#此节点的分类情况</span></span><br><span class="line">    <span class="keyword">if</span> classlist.count(classlist[<span class="number">0</span>]) == len(classlist):  <span class="comment">#全部属于一类</span></span><br><span class="line">        <span class="keyword">return</span> classlist[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:         <span class="comment">#分完了,没有属性了</span></span><br><span class="line">        <span class="keyword">return</span> Vote(classlist,weight)       <span class="comment">#少数服从多数</span></span><br><span class="line">    <span class="comment"># 选择一个最优特征进行划分</span></span><br><span class="line">    bestFeat = chooseBestFeat(dataSet,weight,featname)</span><br><span class="line">    bestFeatname = featname[bestFeat]</span><br><span class="line">    <span class="keyword">del</span>(featname[bestFeat])     <span class="comment">#防止下标不准</span></span><br><span class="line">    DecisionTree = &#123;bestFeatname:&#123;&#125;&#125;</span><br><span class="line">    <span class="comment"># 创建分支,先找出所有属性值,即分支数</span></span><br><span class="line">    allvalue = [vec[bestFeat] <span class="keyword">for</span> vec <span class="keyword">in</span> dataSet <span class="keyword">if</span> vec[bestFeat] != <span class="string">'?'</span>]</span><br><span class="line">    specvalue = sorted(list(set(allvalue)))  <span class="comment">#使有一定顺序</span></span><br><span class="line">    UnmissDataSet,Unmissweight = getUnmissDataSet(dataSet, weight, bestFeat)   <span class="comment">#无缺失值数据集及其权重</span></span><br><span class="line">    UnmissSumWeight = sum(Unmissweight)      <span class="comment"># D~</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> specvalue:</span><br><span class="line">        copyfeatname = featname[:]</span><br><span class="line">        Dv,weightVec_v = splitDataSet(dataSet,Unmissweight,bestFeat,v,<span class="keyword">False</span>)   <span class="comment">#返回 此属性为v的所有样本 以及 每个样本的权重</span></span><br><span class="line">        r_v = sum(weightVec_v) / UnmissSumWeight          <span class="comment">#r~_v = D~_v / D~</span></span><br><span class="line">        sondataSet,sonweight = splitDataSet_adjustWeight(dataSet,weight,bestFeat,v,r_v)</span><br><span class="line">        DecisionTree[bestFeatname][v] = createDecisionTree(sondataSet,sonweight,copyfeatname)</span><br><span class="line">    <span class="keyword">return</span> DecisionTree</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    filename = <span class="string">"D:\\MLinAction\\Data\\西瓜2.0alpha.txt"</span></span><br><span class="line">    DataSet,featname = DataSetPredo(filename,[])</span><br><span class="line">    Tree = createDecisionTree(DataSet,[<span class="number">1.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(DataSet))],featname)</span><br><span class="line">    print(Tree)</span><br></pre></td></tr></table></figure></p>
<p>有缺失值的情况如 <a href="http://whatbeg.com/2016/04/22/xiguadataset.html">西瓜数据集2.0alpha</a><br>实验结果：<br><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'纹理'</span>: &#123;<span class="string">'模糊'</span>: &#123;<span class="string">'色泽'</span>: &#123;<span class="string">'青绿'</span>: <span class="string">'否'</span>, <span class="string">'浅白'</span>: <span class="string">'否'</span>, <span class="string">'乌黑'</span>: <span class="string">'是'</span>&#125;&#125;, <span class="string">'清晰'</span>: &#123;<span class="string">'根蒂'</span>: &#123;<span class="string">'蜷缩'</span>: <span class="string">'是'</span>, <span class="string">'硬挺'</span>: <span class="string">'否'</span>, <span class="string">'稍蜷'</span>: &#123;<span class="string">'色泽'</span>: &#123;<span class="string">'青绿'</span>: <span class="string">'是'</span>, <span class="string">'乌黑'</span>: &#123;<span class="string">'触感'</span>: &#123;<span class="string">'硬滑'</span>: <span class="string">'是'</span>, <span class="string">'软粘'</span>: <span class="string">'否'</span>&#125;&#125;&#125;&#125;&#125;&#125;, <span class="string">'稍糊'</span>: &#123;<span class="string">'敲声'</span>: &#123;<span class="string">'浊响'</span>: &#123;<span class="string">'脐部'</span>: &#123;<span class="string">'凹陷'</span>: <span class="string">'否'</span>, <span class="string">'稍凹'</span>: <span class="string">'是'</span>&#125;&#125;, <span class="string">'清脆'</span>: <span class="string">'否'</span>, <span class="string">'沉闷'</span>: <span class="string">'否'</span>&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<p><img src="http://7xsl28.com2.z0.glb.clouddn.com/dectree3.png" alt="决策树算法在西瓜数据集2.0$\alpha$上的结果"></p>
<h2 id="在乳腺癌数据集上的测试与表现"><a href="#在乳腺癌数据集上的测试与表现" class="headerlink" title="在乳腺癌数据集上的测试与表现"></a>在乳腺癌数据集上的测试与表现</h2><p>有了算法，我们当然想做一定的测试看一看算法的表现。这里我选择了威斯康辛女性乳腺癌的数据。</p>
<p>数据总共有9列，每一列分别代表，以逗号分割<br>1 Sample code number （病人ID）<br>2 Clump Thickness 肿块厚度<br>3 Uniformity of Cell Size 细胞大小的均匀性<br>4 Uniformity of Cell Shape 细胞形状的均匀性<br>5 Marginal Adhesion 边缘粘<br>6 Single Epithelial Cell Size 单上皮细胞的大小<br>7 Bare Nuclei 裸核<br>8 Bland Chromatin 乏味染色体<br>9 Normal Nucleoli 正常核<br>10 Mitoses 有丝分裂<br>11 Class: 2 for benign, 4 formalignant（恶性或良性分类）<br>[from Toby]<br>总共700条左右的数据，选取最后80条作为测试集，前面作为训练集，进行学习。<br>使用分类器的代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> treesID3 <span class="keyword">as</span> id3</span><br><span class="line"><span class="keyword">import</span> treePlot <span class="keyword">as</span> tpl</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(Tree, featnames, X)</span>:</span></span><br><span class="line">    classLabel = <span class="string">"未知"</span></span><br><span class="line">    root = list(Tree.keys())[<span class="number">0</span>]</span><br><span class="line">    firstGen = Tree[root]</span><br><span class="line">    featindex = featnames.index(root)  <span class="comment">#根节点的属性下标</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> firstGen.keys():   <span class="comment">#根属性的取值,取哪个就走往哪颗子树</span></span><br><span class="line">        <span class="keyword">if</span> X[featindex] == key:</span><br><span class="line">            <span class="keyword">if</span> type(firstGen[key]) == type(&#123;&#125;):</span><br><span class="line">                classLabel = classify(firstGen[key],featnames,X)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                classLabel = firstGen[key]</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">StoreTree</span><span class="params">(Tree,filename)</span>:</span></span><br><span class="line">    fw = open(filename,<span class="string">'wb'</span>)</span><br><span class="line">    pickle.dump(Tree,fw)</span><br><span class="line">    fw.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReadTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">    fr = open(filename,<span class="string">'rb'</span>)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    filename = <span class="string">"D:\\MLinAction\\Data\\breastcancer.txt"</span></span><br><span class="line">    dataSet,featnames = id3.DataSetPredo(filename,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line">    Tree = id3.createDecisionTree(dataSet[:<span class="number">620</span>],[<span class="number">1.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(dataSet))],featnames)</span><br><span class="line">    tpl.createPlot(Tree)</span><br><span class="line">    storetree = <span class="string">"D:\\MLinAction\\Data\\decTree.dect"</span></span><br><span class="line">    StoreTree(Tree,storetree)</span><br><span class="line">    <span class="comment">#Tree = ReadTree(storetree)</span></span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    cnt = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> lis <span class="keyword">in</span> dataSet[<span class="number">620</span>:]:</span><br><span class="line">        judge = classify(Tree,featnames,lis[:<span class="number">-1</span>])</span><br><span class="line">        shouldbe = lis[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> judge == shouldbe:</span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line">        print(<span class="string">"Test %d was classified %s, it's class is %s %s"</span> %(i,judge,shouldbe,<span class="string">"====="</span> <span class="keyword">if</span> judge==shouldbe <span class="keyword">else</span> <span class="string">""</span>))</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    print(<span class="string">"The Tree's Accuracy is %.3f"</span> % (cnt / float(i)))</span><br></pre></td></tr></table></figure></p>
<p>结果（点击右键看大图）<br><img src="http://7xsl28.com2.z0.glb.clouddn.com/dectree4.png" alt="在乳腺癌数据上构建决策树"><br><img src="http://7xsl28.com2.z0.glb.clouddn.com/canceroutput.png" alt="测试结果"><br>正确率约为96%左右，算是不差的分类器了。<br>我的乳腺癌数据见：<a href="http://7xt9qk.com2.z0.glb.clouddn.com/breastcancer.txt" target="_blank" rel="external">乳腺癌数据</a><br>至此，决策树算法ID3的实现完毕，下面考虑基于基尼指数和信息增益率进行划分选择，以及考虑实现剪枝过程，因为我们可以看到上面训练出的决策树还存在着很多冗余分支，是因为实现过程中，由于数据量太大，每个分支都不完全纯净，所以会创建往下的分支，但是分支投票的结果又是一致的，而且数据量再大，特征数再多的话，决策树会非常大非常复杂，所以剪枝一般是必做的一步。剪枝分为先剪枝和后剪枝，如果细说的话可以写很多了。<br>利用matplotlib画决策树的代码见<a href="http://whatbeg.com/2016/04/23/matplotlib-desiciontree.html">利用matplotlib画决策树</a><br>此文亦可见：<a href="http://www.cnblogs.com/whatbeg/p/5424890.html" target="_blank" rel="external">Here</a><br>MathJax的一些使用方法见<a href="http://iori.sinaapp.com/17.html/comment-page-1?replytocom=2" target="_blank" rel="external">1</a><a href="http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" target="_blank" rel="external">2</a><a href="http://www.ibiblio.org/koine/greek/lessons/alphabet.html" target="_blank" rel="external">3</a><br>参考资料：《机器学习》《机器学习实战》通过本次实战也发现了这两本书中的一些错误之处。<br>lz初学机器学习不久，如有错漏之处请多包涵指出或者各位有什么想法或意见欢迎评论去告诉我:-)</p>
  
	</div>


    
	<!-- css -->
	<style type="text/css">
	    .center {
	        text-align: center;
	    }
	    .hidden {
	        display: none;
	    }
		.donate_bar a.btn_donate{
			display: inline-block;
			width: 82px;
			height: 82px;
			background: url("http://7xsl28.com1.z0.glb.clouddn.com/btn_reward.gif") no-repeat;
			_background: url("http://7xsl28.com1.z0.glb.clouddn.com/btn_reward.gif") no-repeat;

			<!-- http://img.t.sinajs.cn/t5/style/images/apps_PRF/e_media/btn_reward.gif
			     因为本 hexo 生成的博客所用的 theme 的 a:hover 带动画效果，
				 为了在让打赏按钮显示效果正常 而 添加了以下几行 css，
				 嵌入其它博客时不一定要它们。 -->
			-webkit-transition: background 0s;
			-moz-transition: background 0s;
			-o-transition: background 0s;
			-ms-transition: background 0s;
			transition: background 0s;
			<!-- /让打赏按钮的效果显示正常 而 添加的几行 css 到此结束 -->
		}

		.donate_bar a.btn_donate:hover{ background-position: 0px -82px;}
		.donate_bar .donate_txt {
			display: block;
			color: #9d9d9d;
			font: 14px/2 "Microsoft Yahei";
		}
		.bold{ font-weight: bold; }
	</style>
	<!-- /css -->

    <!-- Donate Module -->
    <div id="donate_module">

	<!-- btn_donate & tips -->
	<div id="donate_board" class="donate_bar center">
	    <br>
	    ------------------------------------------------------------------------------------------------------------------------------
	    <br>
		<a id="btn_donate" class="btn_donate" target="_self" href="javascript:;" title="Donate 打赏"></a>
		<span class="donate_txt">
			我要小额赞助，助作者写出更好的文章！
		</span>
			
		
	</div>
	<!-- /btn_donate & tips -->

	<!-- donate guide -->
    
	<div id="donate_guide" class="donate_bar center hidden">
        <br>
	    ------------------------------------------------------------------------------------------------------------------------------
	    <br>
	    
	    <div width="100%" align="center"><div name="dashmain" id="dash-main-id-87895f" class="dash-main-3 87895f-0.99"></div></div>
		<script type="text/javascript" charset="utf-8" src="http://www.dashangcloud.com/static/ds.js"></script>
		

		<a href="http://7xsl28.com1.z0.glb.clouddn.com/wechatpay.png" title="用微信扫一扫哦~" class="fancybox" rel="article0">
			<img src="http://7xsl28.com1.z0.glb.clouddn.com/wechatpay.png" title="微信打赏 Donate" height="190px" width="auto"/>
		</a>
        
        &nbsp;&nbsp;

		<a href="http://7xsl28.com1.z0.glb.clouddn.com/alipay.jpg" title="用支付宝扫一扫即可~" class="fancybox" rel="article0">
			<img src="http://7xsl28.com1.z0.glb.clouddn.com/alipay.jpg" title="支付宝打赏 Donate" height="190px" width="auto"/>
		</a>

		<span class="donate_txt">
			我要小额赞助，助作者写出更好的文章！
		</span>

	</div>
	<!-- /donate guide -->

	<!-- donate script -->
	<script type="text/javascript">
		document.getElementById('btn_donate').onclick = function() {
			$('#donate_board').addClass('hidden');
			$('#donate_guide').removeClass('hidden');
		}

		function donate_on_web(){
			$('#donate').submit();
        }

		var original_window_onload = window.onload;
        window.onload = function () {
            if (original_window_onload) {
                original_window_onload();
            }
            document.getElementById('donate_board_wdg').className = 'hidden';
		}
	</script>
	<!-- /donate script -->
</div>
<!-- /Donate Module -->
   

		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  
  <span></span> <a href="/categories/机器学习-Mac-Learning/">机器学习 | Mac.Learning</a>
  </div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a><a href="/tags/Python/">Python</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	<div class="share-jiathis">
	  
<div class="jiathis_style_24x24">
	<a class="jiathis_button_tsina"></a>
	<a class="jiathis_button_weixin"></a>
	<a class="jiathis_button_tqq"></a>
	<a class="jiathis_button_qzone"></a>
	<a class="jiathis_button_douban"></a>
	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
    var jiathis_config={
    data_track_clickback:true,
    sm:"copy,renren,cqq",
    pic:"",
    summary:"",
     ralateuid:{"tsina":"husuche## e.g. 2176287895 Your weibo id,It will be used in share button."},hideMore:false}
    
  </script> 
<script type="text/javascript" src="//v3.jiathis.com/code/jia.js?uid=2094149
2094149" charset="utf-8"></script>      

	 </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2016/04/25/latexsyntax.html" title="Latex/MathJax公式的一些语法">
  <strong>上一篇：</strong><br/>
  <span>
  Latex/MathJax公式的一些语法</span>
</a>
</div>


<div class="next">
<a href="/2016/04/23/matplotlib-desiciontree.html"  title="利用matplotlib画决策树">
 <strong>下一篇：</strong><br/> 
 <span>利用matplotlib画决策树
</span>
</a>
</div>

</nav>

	

<section id="comments" class="comment">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <aside class="clearfix">
  <strong class="toc-title">文章目录</strong>
 
 <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树简介"><span class="toc-number">1.</span> <span class="toc-text">决策树简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ID3算法简介"><span class="toc-number">2.</span> <span class="toc-text">ID3算法简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ID3算法实现（纯标称值）"><span class="toc-number">3.</span> <span class="toc-text">ID3算法实现（纯标称值）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#有连续值的情况"><span class="toc-number">4.</span> <span class="toc-text">有连续值的情况</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#有缺失值的情况"><span class="toc-number">5.</span> <span class="toc-text">有缺失值的情况</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#在乳腺癌数据集上的测试与表现"><span class="toc-number">6.</span> <span class="toc-text">在乳腺癌数据集上的测试与表现</span></a></li></ol>
 
 </aside>
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="about">
    <p class="asidetitle">Short bio</p>
    <div class="clearfix">
        <!-- <img src="http://7xsl28.com1.z0.glb.clouddn.com/niuzai.jpg" height="74px" width="74px" id="about-image" alt> -->
        <span style="font-size: medium; font-family: Calibri Light, Open Sans, Microsoft YaHei Light">
        whatbeg.com is written by Qiu Hu. He is living in Nanjing, P.R. China.
        <br>
        You can contact Qiu Hu with email.
        <br>
        Just enjoy your reading here!
        <br>
        Comments are always welcome:)
        <br>
        Some interesting non-technical articles can be seen in his WeChat official account: 
        </span>
        <br>
        <img src="https://gitee.com/whyseek/blogimages/raw/master/qrcode.jpg" height="124px" width="124px" id="qrcode" alt>
    </div>
</div>


  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/大数据系统与技术-Big-Data/" title="大数据系统与技术 | Big Data">大数据系统与技术 | Big Data<sup>11</sup></a></li>
		  
		
		  
			<li><a href="/categories/成长之路-Biography/" title="成长之路 | Biography">成长之路 | Biography<sup>9</sup></a></li>
		  
		
		  
			<li><a href="/categories/数据科学-Data-Science/" title="数据科学 | Data Science">数据科学 | Data Science<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/机器学习-Mac-Learning/" title="机器学习 | Mac.Learning">机器学习 | Mac.Learning<sup>9</sup></a></li>
		  
		
		  
			<li><a href="/categories/深度学习-Deep-Learning/" title="深度学习 | Deep Learning">深度学习 | Deep Learning<sup>7</sup></a></li>
		  
		
		  
			<li><a href="/categories/算法-Algorithm/" title="算法 | Algorithm">算法 | Algorithm<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/编程语言-Program-Lang/" title="编程语言 | Program Lang.">编程语言 | Program Lang.<sup>4</sup></a></li>
		  
		
		  
			<li><a href="/categories/计算机相关-CS-Related/" title="计算机相关 | CS.Related">计算机相关 | CS.Related<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/译文-Translation/" title="译文 | Translation">译文 | Translation<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/读书-Reading/" title="读书 | Reading">读书 | Reading<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/错误解决与优化-Err-Opt/" title="错误解决与优化 | Err&amp;Opt">错误解决与优化 | Err&amp;Opt<sup>13</sup></a></li>
		  
		
		  
			<li><a href="/categories/随笔-Essays/" title="随笔 | Essays">随笔 | Essays<sup>7</sup></a></li>
		  
		
		</ul>
</div>


  
  <div class="archiveslist">
    <p class="asidetitle"><a href="/archives">归档</a></p>
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">二月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">一月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">十二月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">十月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">七月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">十二月 2016</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">九月 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">八月 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">七月 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">六月 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">五月 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">四月 2016</a><span class="archive-list-count">23</span></li></ul>
  </div>


  
  <div class="tagcloudlist">
    <p class="asidetitle">标签云</p>
    <div class="tagcloudlist clearfix">
       <a href="/tags/CS/" style="font-size: 17px;">CS</a> <a href="/tags/Deep-Learning/" style="font-size: 17px;">Deep Learning</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/Git/" style="font-size: 11px;">Git</a> <a href="/tags/Hadoop/" style="font-size: 14px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Latex/" style="font-size: 11px;">Latex</a> <a href="/tags/Linux/" style="font-size: 13px;">Linux</a> <a href="/tags/Python/" style="font-size: 19px;">Python</a> <a href="/tags/Scala/" style="font-size: 10px;">Scala</a> <a href="/tags/Spark/" style="font-size: 12px;">Spark</a> <a href="/tags/Summary/" style="font-size: 15px;">Summary</a> <a href="/tags/TensorFlow/" style="font-size: 12px;">TensorFlow</a> <a href="/tags/Web/" style="font-size: 12px;">Web</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/三国/" style="font-size: 10px;">三国</a> <a href="/tags/人文社科/" style="font-size: 10px;">人文社科</a> <a href="/tags/优化/" style="font-size: 10px;">优化</a> <a href="/tags/吃喝玩乐/" style="font-size: 10px;">吃喝玩乐</a> <a href="/tags/大数据/" style="font-size: 16px;">大数据</a> <a href="/tags/工具使用/" style="font-size: 10px;">工具使用</a> <a href="/tags/影视/" style="font-size: 11px;">影视</a> <a href="/tags/数据分析/" style="font-size: 16px;">数据分析</a> <a href="/tags/机器学习/" style="font-size: 20px;">机器学习</a> <a href="/tags/深度学习/" style="font-size: 18px;">深度学习</a> <a href="/tags/算法/" style="font-size: 12px;">算法</a> <a href="/tags/系统/" style="font-size: 10px;">系统</a> <a href="/tags/论文阅读/" style="font-size: 12px;">论文阅读</a> <a href="/tags/译文/" style="font-size: 12px;">译文</a> <a href="/tags/读书/" style="font-size: 18px;">读书</a> <a href="/tags/错误解决/" style="font-size: 12px;">错误解决</a> <a href="/tags/随笔/" style="font-size: 15px;">随笔</a>
    </div>
  </div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="http://www.cnblogs.com/whatbeg/" target="_blank" title="Old Blog">Old Blog</a>
            
          </li>
        
          <li>
            
            	<a href="https://github.com/whatbeg" target="_blank" title="My Github">My Github</a>
            
          </li>
        
          <li>
            
            	<a href="http://www.matrix67.com/blog/" target="_blank" title="Matrix67">Matrix67</a>
            
          </li>
        
          <li>
            
            	<a href="http://www.liaoxuefeng.com/" target="_blank" title="廖雪峰">廖雪峰</a>
            
          </li>
        
          <li>
            
            	<a href="http://www.ruanyifeng.com/blog/" target="_blank" title="阮一峰">阮一峰</a>
            
          </li>
        
          <li>
            
            	<a href="http://mindhacks.cn/" target="_blank" title="刘未鹏">刘未鹏</a>
            
          </li>
        
          <li>
            
            	<a href="http://machinelearningmastery.com/blog/" target="_blank" title="ML MYSTERY">ML MYSTERY</a>
            
          </li>
        
          <li>
            
            	<a href="http://freemind.pluskid.org/" target="_blank" title="Free Mind">Free Mind</a>
            
          </li>
        
          <li>
            
            	<a href="http://blog.csdn.net/zouxy09/" target="_blank" title="zouxy机器学习">zouxy机器学习</a>
            
          </li>
        
          <li>
            
            	<a href="http://coolshell.cn" target="_blank" title="酷壳">酷壳</a>
            
          </li>
        
    </ul>
</div>

  <table height=30 cellSpacing=0 cellPadding=0 width=180 border=0>
<form action="http://www.sogou.com/web" target="_blank">
<tr style='font-size:12px;color:#000000'>
<td align="center" width=100><input type="text" name="query" size=14 style='BORDER-RIGHT: #999 1px solid; BORDER-TOP: #999 1px solid; BORDER-LEFT: #999	1px	solid; BORDER-BOTTOM: #999 1px solid; HEIGHT: 19px; BACKGROUND-COLOR: #fff'>
<input type="hidden" name="insite" value="whatbeg.com">
<input type="hidden" name="insite2" value="whatbeg.com"></td>
<td align="left" width=45><input type="submit" name="sogou_submit" value="搜索">
</td></tr></form>
</table>



  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

  <div class="rsspart">
	<a href="http://eepurl.com/cHO5An" target="_blank" title="email">Email 订阅</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
        

	    
		
				<div class="cc-license">
          <a href="http://creativecommons.org/licenses/by-nc-nd/4.0" class="cc-opacity" target="_blank">
            <img src="/img/cc-by-nc-nd.svg" alt="Creative Commons" />
          </a>
        </div>
    

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2016-2018 
		
		<a href="/about" target="_blank" title="whatbeg">whatbeg</a>
		
		<br>
		<span class="post-count">Total words: <span style="color:orange">320.1k</span></span>
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
        <span id="busuanzi_container_site_pv">
        &nbsp;&nbsp;Total visits:&nbsp;<span style="color:orange" id="busuanzi_value_site_pv"></span>
        </span>
        <span id="busuanzi_container_site_uv">
        &nbsp;&nbsp;You are Visitor No.<span style="color:orange" id="busuanzi_value_site_uv"></span>
        </span>
        </br>
		</p>
		
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    $('#toc.toc-aside').css('display', 'block').addClass('fadeIn');  //侧边栏显示文章目录
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');   //侧边栏显示widget
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
        getSize();
        if (myWidth >= 1024) {
          c.click();
        }
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>





<script type="text/javascript">

var disqus_shortname = 'whatbeg';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>






<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e6d1f421bbc9962127a50488f9ed37d1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1258390595'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1258390595%26online%3D1%26show%3Dline' type='text/javascript'%3E%3C/script%3E"));</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->
<script>
(function(){
    var bp = document.createElement('script');
    bp.src = '//push.zhanzhang.baidu.com/push.js';
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
  </body>
</html>


