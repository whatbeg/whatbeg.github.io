
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>Training Classifiers via Stochastic Gradient Descent | Whatbeg&#39;s blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="whatbeg">
    

    
    <meta name="description" content="AbstractThis report states how I implements two algorithm: Logistic Regression and Ridge Regression(Classifier) which are tailored to large-scale classification. Given the datasets with a large number">
<meta property="og:type" content="article">
<meta property="og:title" content="Training Classifiers via Stochastic Gradient Descent">
<meta property="og:url" content="http://whatbeg.com/2016/11/18/trainclassifiersviaSGD.html">
<meta property="og:site_name" content="Whatbeg's blog">
<meta property="og:description" content="AbstractThis report states how I implements two algorithm: Logistic Regression and Ridge Regression(Classifier) which are tailored to large-scale classification. Given the datasets with a large number">
<meta property="og:image" content="http://7xsl28.com1.z0.glb.clouddn.com/SGDalgorithm1.png">
<meta property="og:image" content="http://7xsl28.com1.z0.glb.clouddn.com/LRfig1.png">
<meta property="og:image" content="http://7xsl28.com1.z0.glb.clouddn.com/LRfig2.png">
<meta property="og:image" content="http://7xsl28.com1.z0.glb.clouddn.com/SGDalgorithm2.png">
<meta property="og:image" content="http://7xsl28.com1.z0.glb.clouddn.com/RRfig3.png">
<meta property="og:image" content="http://7xsl28.com1.z0.glb.clouddn.com/RRfig4.png">
<meta property="og:image" content="http://7xsl28.com1.z0.glb.clouddn.com/LRfig5.png">
<meta property="og:image" content="http://7xsl28.com1.z0.glb.clouddn.com/LRfig6.png">
<meta property="og:image" content="http://7xsl28.com1.z0.glb.clouddn.com/refernce1.png">
<meta property="og:updated_time" content="2016-11-19T02:53:16.033Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Training Classifiers via Stochastic Gradient Descent">
<meta name="twitter:description" content="AbstractThis report states how I implements two algorithm: Logistic Regression and Ridge Regression(Classifier) which are tailored to large-scale classification. Given the datasets with a large number">
<meta name="twitter:image" content="http://7xsl28.com1.z0.glb.clouddn.com/SGDalgorithm1.png">

    
    <link rel="alternative" href="/atom.xml" title="Whatbeg&#39;s blog" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/w.ico">
    
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Whatbeg&#39;s blog">Whatbeg&#39;s blog</a></h1>
				<h2 class="blog-motto">当你的才华撑不起你的野心时，就应该静下心来好好学习。</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">首页(Home)</a></li>
					
						<li><a href="/archives">归档(Archives)</a></li>
					
						<li><a href="/tags">标签(Tags)</a></li>
					
						<li><a href="/categories">分类(Categories)</a></li>
					
						<li><a href="/about">关于(About)</a></li>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/11/18/trainclassifiersviaSGD.html" title="Training Classifiers via Stochastic Gradient Descent" itemprop="url">Training Classifiers via Stochastic Gradient Descent</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="whatbeg" target="_blank" itemprop="author">whatbeg</a>
		
  <p class="article-time">
    <time datetime="2016-11-18T07:33:51.000Z" itemprop="datePublished"> 发表于 2016-11-18</time>
    <span id="busuanzi_container_page_pv">
    总阅读<span id="busuanzi_value_page_pv"></span>次
    </span>
  </p>

</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction-to-Datasets"><span class="toc-number">2.</span> <span class="toc-text">Introduction to Datasets</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression-via-SGD-with-L1-Regularization"><span class="toc-number">3.</span> <span class="toc-text">Logistic Regression via SGD with L1-Regularization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#What-is-Logistic-Regression"><span class="toc-number">3.1.</span> <span class="toc-text">What is Logistic Regression?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation"><span class="toc-number">3.2.</span> <span class="toc-text">Implementation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Results-of-Experiment"><span class="toc-number">3.3.</span> <span class="toc-text">Results of Experiment</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Comparison-of-Naive-and-Cumulative-Penalty-L1-regularization-in-LR"><span class="toc-number">3.4.</span> <span class="toc-text">Comparison of Naive and Cumulative Penalty L1 regularization in LR</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ridge-Regression-via-SGD-with-L2-regularization"><span class="toc-number">4.</span> <span class="toc-text">Ridge Regression via SGD with L2-regularization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation-1"><span class="toc-number">4.1.</span> <span class="toc-text">Implementation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Results-of-Experiment-1"><span class="toc-number">4.2.</span> <span class="toc-text">Results of Experiment</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Results-Appendix"><span class="toc-number">5.</span> <span class="toc-text">Results Appendix</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number">6.</span> <span class="toc-text">Reference</span></a></li></ol>
		
		</div>
		
		<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>This report states how I implements two algorithm: Logistic Regression and Ridge Regression(Classifier) which are tailored to large-scale classification. Given the datasets with a large number of examples, we will train classifiers via stochastic gradient descent(SGD) in order to make training phase more efficient. </p>
<h2 id="Introduction-to-Datasets"><a href="#Introduction-to-Datasets" class="headerlink" title="Introduction to Datasets"></a>Introduction to Datasets</h2><p>There are two datasets which come from LIBSVM, Dataset 1(Training) contains 32561 rows and 124 columns, and dataset 1(Testing) contains 16281 rows and 124 columns; Dataset 2 (Training) contains 290507 rows and 54 columns, and dataset 2 (Testing) contains 290505 rows and 54 columns. Each row represents an example. The last column represents the label of the corresponding example, and the remaining columns represent the features of the corresponding example. For each dataset, label $\mathcal{Y} = {-1, +1}$. After carefully observing the datasets, I find Dataset 1 contains only 0 or 1, but Dataset 2 contains both float numbers between 0 and 1 and integer 0 or 1, indicating that Dataset 2 may not be scaled, so I scaled then to [0, 1], using Min-Max scaling, fortunately Dataset 1 was immune to the scaling because of above description.</p>
<h2 id="Logistic-Regression-via-SGD-with-L1-Regularization"><a href="#Logistic-Regression-via-SGD-with-L1-Regularization" class="headerlink" title="Logistic Regression via SGD with L1-Regularization"></a>Logistic Regression via SGD with L1-Regularization</h2><p>Logistic Regression is one of most popular classification algorithm in our daily life and industrial application. In this section, I will show you a profile of Logistic Regression at first, and then give the pseudo code of my implementation, then I will show results in two datasets after run my code, and compare it to a python machine learning library Scikit-Learn’s results, and at last show some figures of results for you understanding the whole process.</p>
<h3 id="What-is-Logistic-Regression"><a href="#What-is-Logistic-Regression" class="headerlink" title="What is Logistic Regression?"></a>What is Logistic Regression?</h3><p>In linear regression, we note the model as $$y = w^Tx + b$$ but can we let predictions approach some thing derive from y ? Answer is absolutely yes, for example we can use log of y as objective that linear models should approach to, which is called log-linear regression.<br>$$\log{y} = w^Tx + b$$ Generally, consider a differentiable monotone function $ g(\cdot) $, let $$y = g^{-1}(w^Tx+b)$$ models like above form is called generalized linear model. Logistic regression can be seen as a special case of the generalized linear model and thus analogous to<br>linear regression. The binary logistic model is used to estimate the probability of a binary response based on one or more predictor (or independent) variables (features),[@wiki] it’s $ g^{-1}(\cdot) $ is $$y = \frac {1} {1 + e^{-w^Tx+b} }$$ above equation is actually using prediction results of linear regression model to approach logit probability of true label, thus, the corresponding model is called “Logistic Regression”, although it’s name contains “Regression”, it is truly a classifier. Logistic regression have many advantages in classification task, including that it directly modeling on the real data, unnecessary to make hypothesis of data distribution, which avoided problems with inaccurate hypothesis of data distribution, moreover, it not only predict “categories”, but approximation of categories’ probability, last but not least, logistic function is a convex function which have many merits.<br>We can do some conversion Eq.4 that: $$\ln{\frac {y} {1-y} } = w^Tx+b$$ and we can see y as posterior probability estimates $ p(y=1|x) $, so Eq.5 can be rewrite as $$\ln{\frac {p(y=1|x)} {p(y=-1|x)} } = w^Tx+b$$ absolutely we can infer following two equation:<br>$$p(y=1|x) = \frac{e^{w^Tx+b} } {1+e^{w^Tx+b} } = \frac{1}{1+e^{-(w^Tx+b)} }$$<br>$$p(y=1|x) = \frac{1}{1+e^{w^Tx+b} }$$ thus we can use maximum likelihood<br>method to estimate $w$ and $b$, and for convenience, we note $ \beta = (b;w) $ and $ x = (1;x) $, and we add L1-regularization to our loss function, so our loss function can be written as</p>
<p>$$ {Loss = \frac{1}{N} \sum_{i=1}^N {\log(1+e^{-y_i\beta^Tx_i}) }  + \lambda||\beta||_1} $$</p>
<p>after add L1-regularization, we note the final goal is to</p>
<p>$$ min {\frac{1}{N} \sum_{i=1}^N {\log(1+e^{-y_i\beta^Tx_i}) }  + \lambda||\beta||_1} $$</p>
<p>Stochastic Gradient Descent method usually used to solve the above optimization problem, which from Gradient Descent, because of GD’s low efficiency in large scale dataset, people want to speed up the rate of<br>convergence, at the same time without losing accuracy, so, they use a randomly selected every time to update parameters, that is to say, the batch size(the number of training examples) used for approximation of Eq.10 is 1. Using SGD to solve Eq.10, we first calculate the gradient of Eq.10 for a<br>sample $x_i$:<br>$$\frac {\partial{loss(\beta, x_i)} }{ {\partial\beta} } = \frac{-y_ix_i} {1+e^{y_i\beta^Tx_i} } + \lambda sign(\beta) $$</p>
<p>so we can update weights $\beta$ with every iteration and every data</p>
<p>sample using following equation:</p>
<p>$$\beta_{k+1} = \beta_k - \gamma \frac {\partial{loss(\beta, x_i)} } {\partial\beta}<br>                = \beta_k + \gamma\frac{y_ix_i} {1+e^{y_i\beta^Tx_i} } - \gamma\lambda sign(\beta) $$</p>
<p>where $sign(x) = 1$ if x &gt; 0, $sign(x) = -1$, and $sign(x) = 0$ if x = 0. This update equation was called “SGD-L1(Naive)”[@cumulative], and this naive method will cause two problems, one is that at each update, we need to perform the application of L1 penalty to all features, including the features that are not used in current training sample, and it does not produce a compact model, that is, when a dimension of $\beta$ is very small and we can neglect it’s impact, but we will update that dimension as 1 or -1 times regularization lambda and learning rate, regardless of it’s size, at next update, this will cause appearance of many non-zero value in $\beta$, but we want that with the same effect, the less features the better.</p>
<blockquote>
<p><strong>Occam’s Razor: Entities should not be multiplied unnecessarily.</strong></p>
</blockquote>
<p>For this reason, I tried some methods in Yoshimasa’s paper, including Clipping, which clip the weight that crosses zero, and the update function can refer his paper, and finally I used L1 regularization with cumulative penalty to apply L1 penalty to object function, any details can be find in his paper.[@cumulative]</p>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>After above analysis and derivation, we can easily write the algorithm using SGD to solve logistic regression with L1 regularization,($wh$ is a temporary variable). </p>
<p><img src="http://7xsl28.com1.z0.glb.clouddn.com/SGDalgorithm1.png" alt=""></p>
<p>In order to evaluate effectiveness of my algorithm implementation, I written a implementation using<br>Scikit-Learn, a python machine learning library, and you can see the comparison in Table.2. After conduct a simple Cross Validation and adopted some tricks[@leonbottou] on dataset, I set hyperparameters as following values:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Hypermeters</th>
<th style="text-align:center">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Learning Rate $\gamma$</td>
<td style="text-align:center">0.001</td>
</tr>
<tr>
<td style="text-align:center">Regularization Lambda $\lambda$</td>
<td style="text-align:center">0.0001</td>
</tr>
<tr>
<td style="text-align:center">Max Iteration Times on Dataset</td>
<td style="text-align:center">200(Dataset1), 30(Dataset2)</td>
</tr>
</tbody>
</table>
<h3 id="Results-of-Experiment"><a href="#Results-of-Experiment" class="headerlink" title="Results of Experiment"></a>Results of Experiment</h3><p>After the programming, I evaluated the effectiveness and accuracy of my training algorithm on all 2 datasets, Figure.1 and Figure.2 show the change of key dependent variable including Train set error rate, Test error rate, Objective function and Sparsity in training process.<br>Figure.1 shows results of logistic regression with L1-regularization on dataset 1 that with increasing value of iteration times of training, the training error rate and test set error rate decrease, like one line, after nearly 500000 times of iteration, the training error and test set<br>error keep stable and smooth, only have some small fluctuations, indicating the loss function converged. Objective function has the same trend. See the red line indicating that with the increasing of training iteration, sparsity of weights vector increasing continually, final results after more than 6000000+ iterations, weights vector have sparsity of 31 dimension of 124 dimension, holding a quarter, that is to say, we just need use 3/4 of origin features to predict class labels of data sample, without loss of accuracy of prediction, see, we did a feature selection unconsciously.</p>
<p><img src="http://7xsl28.com1.z0.glb.clouddn.com/LRfig1.png" alt=""></p>
<p>Figure.2 shows results of conducting algorithm on dataset 2, and we can see almost the same trend on Figure.1, training error rate, test set error rate and objective function decrease with increasing of iteration times, and sparsity increase. What different is that test set error rate on data set 2 is about 0.45, more than stable training error rate, 0.1879, and following form is the final results of logistic regression with L1 regularization on two Dataset.</p>
<p><img src="http://7xsl28.com1.z0.glb.clouddn.com/LRfig2.png" alt=""></p>
<table>
<thead>
<tr>
<th style="text-align:center">Dataset</th>
<th style="text-align:center">Training error rate</th>
<th style="text-align:center">Test error rate</th>
<th style="text-align:center">Sparsity</th>
<th style="text-align:center">Scikit-Learn Test error rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Dataset 1</td>
<td style="text-align:center">0.1504</td>
<td style="text-align:center">0.1501</td>
<td style="text-align:center">31/124</td>
<td style="text-align:center">0.1500</td>
</tr>
<tr>
<td style="text-align:center">Dataset 2</td>
<td style="text-align:center">0.1879</td>
<td style="text-align:center">0.4475</td>
<td style="text-align:center">9/55</td>
<td style="text-align:center">0.4100</td>
</tr>
</tbody>
</table>
<h3 id="Comparison-of-Naive-and-Cumulative-Penalty-L1-regularization-in-LR"><a href="#Comparison-of-Naive-and-Cumulative-Penalty-L1-regularization-in-LR" class="headerlink" title="Comparison of Naive and Cumulative Penalty L1 regularization in LR"></a>Comparison of Naive and Cumulative Penalty L1 regularization in LR</h3><p>As I stated, there are many approach to apply L1 penalty to our logistic regression’s loss function, one of that is so-called “Naive” method[@cumulative], which directly use absolute value of $sign(\beta)$ as it’s sub-gradient for weight update, but I used the cumulative penalty method, and following is the comparison of this two method. From Table.3 and Table.4, we can see that two methods have nearly the same performance, but cumulative penalty method has large sparsity and Naive method has 0 sparsity. See figures of the performance of Naive method in<br>Appendix.</p>
<table>
<thead>
<tr>
<th style="text-align:center">Methods</th>
<th style="text-align:center">Training error rate</th>
<th style="text-align:center">Test error rate</th>
<th style="text-align:center">Sparsity</th>
<th style="text-align:center">Scikit-Learn Test error rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Naive</td>
<td style="text-align:center">0.1507</td>
<td style="text-align:center">0.1493</td>
<td style="text-align:center">0/124</td>
<td style="text-align:center">0.1500</td>
</tr>
<tr>
<td style="text-align:center">Cumulative</td>
<td style="text-align:center">0.1504</td>
<td style="text-align:center">0.1501</td>
<td style="text-align:center">31/124</td>
<td style="text-align:center">0.1500</td>
</tr>
</tbody>
</table>
<p>Table. Comparison of Naive and Cumulative method in Dataset 1</p>
<table>
<thead>
<tr>
<th style="text-align:center">Methods</th>
<th style="text-align:center">Training error rate</th>
<th style="text-align:center">Test error rate</th>
<th style="text-align:center">Sparsity</th>
<th style="text-align:center">Scikit-Learn Test error rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Naive</td>
<td style="text-align:center">0.1878</td>
<td style="text-align:center">0.4467</td>
<td style="text-align:center">0/55</td>
<td style="text-align:center">0.4100</td>
</tr>
<tr>
<td style="text-align:center">Cumulative</td>
<td style="text-align:center">0.1879</td>
<td style="text-align:center">0.4475</td>
<td style="text-align:center">9/55</td>
<td style="text-align:center">0.4100</td>
</tr>
</tbody>
</table>
<p>Table. Comparison of Naive and Cumulative method in Dataset2</p>
<h2 id="Ridge-Regression-via-SGD-with-L2-regularization"><a href="#Ridge-Regression-via-SGD-with-L2-regularization" class="headerlink" title="Ridge Regression via SGD with L2-regularization"></a>Ridge Regression via SGD with L2-regularization</h2><p>Ridge Regression is a variant of Linear Regression, as we all know, there are many different methods to fit the linear model to a set of training data, but by far the most popular is the method of $least~squares$, in this approach, we pick the coefficients $w$ to minimize the residual sum of squares[@elmml] </p>
<p>$$RSS(w) = \sum_{i=1}^N {(y_i - w^Tx_i)^2}$$ </p>
<p>because $RSS(w)$ is a quadratic function of the parameters, its’ minimum always exists, we can use many numerical optimization algorithm to optimize the function, like SGD. But what’s ridge regression, actually it’s very simple, ridge regression shrinks the regression coefficients by imposing a penalty on their size, the ridge coefficients minimize a penalized residual sum of squares </p>
<p>$$min {\frac{1}{N} \sum_{i=1}^N{ (y_i-w^Tx_i)^2 } + \lambda|w|_2^2} $$ </p>
<p>and that, is our objective. Using SGD to solve this optimization problem, we just need to get the gradient of objective in one training sample, which is</p>
<p>$$\nabla loss(w, x_i) = -2x_i(y_i-w^Tx_i) + 2\lambda w$$ So, according Eq.15, we can easily write update function of weights by one iteration and one training example as:</p>
<p>$$w_{k+1} = w_k - \gamma \frac {\partial{loss(w, x_i)} }{ {\partial w} }<br>                = w_k + 2\gamma {x_i(y_i-w^Tx_i)} - 2 \gamma\lambda w$$</p>
<h3 id="Implementation-1"><a href="#Implementation-1" class="headerlink" title="Implementation"></a>Implementation</h3><p>As we have the update function of ridge coefficient, we write it’s pseudo-code easily in Algorithm 2. </p>
<p><img src="http://7xsl28.com1.z0.glb.clouddn.com/SGDalgorithm2.png" alt=""></p>
<p>In order to evaluate effectiveness of my algorithm implementation, I also written a implementation using Scikit-Learn, a python machine learning library, and you can see the comparison in Table.6. And after conduct a simple Cross Validation and use some tricks[@leonbottou]on dataset, I set hyperparameters as following values: ($t$ is iteration times)</p>
<table>
<thead>
<tr>
<th style="text-align:center">Hyperparameters</th>
<th style="text-align:center">Values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Learning Rate $\gamma$</td>
<td style="text-align:center">0.001 / (1.0 + 0.001<em>t</em>0.003)</td>
</tr>
<tr>
<td style="text-align:center">Regularization Lambda $\lambda$</td>
<td style="text-align:center">0.0001</td>
</tr>
<tr>
<td style="text-align:center">Max Iteration Times on Dataset</td>
<td style="text-align:center">200(Dataset1), 30(Dataset2)</td>
</tr>
</tbody>
</table>
<h3 id="Results-of-Experiment-1"><a href="#Results-of-Experiment-1" class="headerlink" title="Results of Experiment"></a>Results of Experiment</h3><p>After the programming, I evaluated the effectiveness and accuracy of my training algorithm on all 2 datasets, Figure.3 and Figure.4 show the change of key dependent variable including Train set error rate, Test error rate, Objective function and Sparsity in training process. Figure.3 shows results of ridge regression with L2-regularization on dataset 1 that with increasing value of iteration times of training, the training error rate and test set error rate decrease, like one line, after nearly 350000 times of iteration, the training error and test set error keep stable and smooth, only have some small fluctuations, indicating the loss function converged. Objective function has the same trend. Because of algorithm applying L2 regularization on loss function,<br>so there will be no sparsity, because of the property of L2 regularization. From here we can know that, L1 regularization will cause more sparsity of weights vector than L2 regularization, which is the main reason of L1 regularization has been adopted/used in many scenarios, no matter in industry or academia. Figure.4 shows results of conducting algorithm on dataset 2, and we can see almost the same trend on Figure.1, training error rate, test set error rate and objective function decrease with increasing of iteration times, and sparsity increase. What different is that test set error rate on data set 2 is about 0.45, more than stable training error rate, 0.1921, and following form is the final results of ridge regression with L2 regularization on two Dataset.</p>
<p><img src="http://7xsl28.com1.z0.glb.clouddn.com/RRfig3.png" alt=""></p>
<p><img src="http://7xsl28.com1.z0.glb.clouddn.com/RRfig4.png" alt=""></p>
<h2 id="Results-Appendix"><a href="#Results-Appendix" class="headerlink" title="Results Appendix"></a>Results Appendix</h2><table>
<thead>
<tr>
<th style="text-align:center">Dataset</th>
<th style="text-align:center">Training error rate</th>
<th style="text-align:center">Test error rate</th>
<th style="text-align:center">Scikit-Learn Test error rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Dataset 1</td>
<td style="text-align:center">0.1547</td>
<td style="text-align:center">0.1548</td>
<td style="text-align:center">0.1500</td>
</tr>
<tr>
<td style="text-align:center">Dataset 2</td>
<td style="text-align:center">0.1921</td>
<td style="text-align:center">0.4449</td>
<td style="text-align:center">0.4100</td>
</tr>
</tbody>
</table>
<p><img src="http://7xsl28.com1.z0.glb.clouddn.com/LRfig5.png" alt=""></p>
<p><img src="http://7xsl28.com1.z0.glb.clouddn.com/LRfig6.png" alt=""></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><img src="http://7xsl28.com1.z0.glb.clouddn.com/refernce1.png" alt=""></p>
  
	</div>


    
	<!-- css -->
	<style type="text/css">
	    .center {
	        text-align: center;
	    }
	    .hidden {
	        display: none;
	    }
		.donate_bar a.btn_donate{
			display: inline-block;
			width: 82px;
			height: 82px;
			background: url("http://7xsl28.com1.z0.glb.clouddn.com/btn_reward.gif") no-repeat;
			_background: url("http://7xsl28.com1.z0.glb.clouddn.com/btn_reward.gif") no-repeat;

			<!-- http://img.t.sinajs.cn/t5/style/images/apps_PRF/e_media/btn_reward.gif
			     因为本 hexo 生成的博客所用的 theme 的 a:hover 带动画效果，
				 为了在让打赏按钮显示效果正常 而 添加了以下几行 css，
				 嵌入其它博客时不一定要它们。 -->
			-webkit-transition: background 0s;
			-moz-transition: background 0s;
			-o-transition: background 0s;
			-ms-transition: background 0s;
			transition: background 0s;
			<!-- /让打赏按钮的效果显示正常 而 添加的几行 css 到此结束 -->
		}

		.donate_bar a.btn_donate:hover{ background-position: 0px -82px;}
		.donate_bar .donate_txt {
			display: block;
			color: #9d9d9d;
			font: 14px/2 "Microsoft Yahei";
		}
		.bold{ font-weight: bold; }
	</style>
	<!-- /css -->

    <!-- Donate Module -->
    <div id="donate_module">

	<!-- btn_donate & tips -->
	<div id="donate_board" class="donate_bar center">
	    <br>
	    ------------------------------------------------------------------------------------------------------------------------------
	    <br>
		<a id="btn_donate" class="btn_donate" target="_self" href="javascript:;" title="Donate 打赏"></a>
		<span class="donate_txt">
			听说，打赏我的人最后都找到了真爱。
		</span>
			
		
	</div>
	<!-- /btn_donate & tips -->

	<!-- donate guide -->
    
	<div id="donate_guide" class="donate_bar center hidden">
        <br>
	    ------------------------------------------------------------------------------------------------------------------------------
	    <br>
	    
	    <div width="100%" align="center"><div name="dashmain" id="dash-main-id-87895f" class="dash-main-3 87895f-0.99"></div></div>
		<script type="text/javascript" charset="utf-8" src="http://www.dashangcloud.com/static/ds.js"></script>
		

		<a href="http://7xsl28.com1.z0.glb.clouddn.com/wechatpay.png" title="用微信扫一扫哦~" class="fancybox" rel="article0">
			<img src="http://7xsl28.com1.z0.glb.clouddn.com/wechatpay.png" title="微信打赏 Donate" height="190px" width="auto"/>
		</a>
        
        &nbsp;&nbsp;

		<a href="http://7xsl28.com1.z0.glb.clouddn.com/alipay.jpg" title="用支付宝扫一扫即可~" class="fancybox" rel="article0">
			<img src="http://7xsl28.com1.z0.glb.clouddn.com/alipay.jpg" title="支付宝打赏 Donate" height="190px" width="auto"/>
		</a>

		<span class="donate_txt">
			听说，打赏我的人最后都找到了真爱。
		</span>

	</div>
	<!-- /donate guide -->

	<!-- donate script -->
	<script type="text/javascript">
		document.getElementById('btn_donate').onclick = function() {
			$('#donate_board').addClass('hidden');
			$('#donate_guide').removeClass('hidden');
		}

		function donate_on_web(){
			$('#donate').submit();
        }

		var original_window_onload = window.onload;
        window.onload = function () {
            if (original_window_onload) {
                original_window_onload();
            }
            document.getElementById('donate_board_wdg').className = 'hidden';
		}
	</script>
	<!-- /donate script -->
</div>
<!-- /Donate Module -->
   

		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  
  <span></span> <a href="/categories/机器学习-Mac-Learning/">机器学习 | Mac.Learning</a>
  </div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	<div class="share-jiathis">
	  
<div class="jiathis_style_24x24">
	<a class="jiathis_button_tsina"></a>
	<a class="jiathis_button_weixin"></a>
	<a class="jiathis_button_tqq"></a>
	<a class="jiathis_button_qzone"></a>
	<a class="jiathis_button_douban"></a>
	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
    var jiathis_config={
    data_track_clickback:true,
    sm:"copy,renren,cqq",
    pic:"",
    summary:"",
     ralateuid:{"tsina":"husuche## e.g. 2176287895 Your weibo id,It will be used in share button."},hideMore:false}
    
  </script> 
<script type="text/javascript" src="//v3.jiathis.com/code/jia.js?uid=2094149
2094149" charset="utf-8"></script>      

	 </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2016/11/26/2016read6month.html" title="近几个月读书总结">
  <strong>上一篇：</strong><br/>
  <span>
  近几个月读书总结</span>
</a>
</div>


<div class="next">
<a href="/2016/11/16/godtianpinyin.html"  title="如何从头开始实现一个中文拼音输入法？">
 <strong>下一篇：</strong><br/> 
 <span>如何从头开始实现一个中文拼音输入法？
</span>
</a>
</div>

</nav>

	

<section id="comments" class="comment">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <aside class="clearfix">
  <strong class="toc-title">文章目录</strong>
 
 <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction-to-Datasets"><span class="toc-number">2.</span> <span class="toc-text">Introduction to Datasets</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression-via-SGD-with-L1-Regularization"><span class="toc-number">3.</span> <span class="toc-text">Logistic Regression via SGD with L1-Regularization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#What-is-Logistic-Regression"><span class="toc-number">3.1.</span> <span class="toc-text">What is Logistic Regression?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation"><span class="toc-number">3.2.</span> <span class="toc-text">Implementation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Results-of-Experiment"><span class="toc-number">3.3.</span> <span class="toc-text">Results of Experiment</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Comparison-of-Naive-and-Cumulative-Penalty-L1-regularization-in-LR"><span class="toc-number">3.4.</span> <span class="toc-text">Comparison of Naive and Cumulative Penalty L1 regularization in LR</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ridge-Regression-via-SGD-with-L2-regularization"><span class="toc-number">4.</span> <span class="toc-text">Ridge Regression via SGD with L2-regularization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation-1"><span class="toc-number">4.1.</span> <span class="toc-text">Implementation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Results-of-Experiment-1"><span class="toc-number">4.2.</span> <span class="toc-text">Results of Experiment</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Results-Appendix"><span class="toc-number">5.</span> <span class="toc-text">Results Appendix</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number">6.</span> <span class="toc-text">Reference</span></a></li></ol>
 
 </aside>
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="about">
    <p class="asidetitle">Short bio</p>
    <div class="clearfix">
        <!-- <img src="http://7xsl28.com1.z0.glb.clouddn.com/niuzai.jpg" height="74px" width="74px" id="about-image" alt> -->
        <span style="font-size: medium; font-family: Calibri Light, Open Sans, Microsoft YaHei Light">
        whatbeg.com is written by Qiu Hu. Now Qiu Hu is a Master Grad. living in Nanjing, Jiangsu Province, China.
        <br>
        You can contact me with email.
        <br>
        Just enjoy your reading here!
        <br>
        Comments are always welcome:)
        </span>
    </div>
</div>


  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/大数据系统与技术-Big-Data/" title="大数据系统与技术 | Big Data">大数据系统与技术 | Big Data<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/成长之路-Biography/" title="成长之路 | Biography">成长之路 | Biography<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/数据科学-Data-Science/" title="数据科学 | Data Science">数据科学 | Data Science<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/机器学习-Mac-Learning/" title="机器学习 | Mac.Learning">机器学习 | Mac.Learning<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/深度学习-Deep-Learning/" title="深度学习 | Deep Learning">深度学习 | Deep Learning<sup>5</sup></a></li>
		  
		
		  
			<li><a href="/categories/算法-Algorithm/" title="算法 | Algorithm">算法 | Algorithm<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/编程语言-Program-Lang/" title="编程语言 | Program Lang.">编程语言 | Program Lang.<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/计算机相关-CS-Related/" title="计算机相关 | CS.Related">计算机相关 | CS.Related<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/译文-Translation/" title="译文 | Translation">译文 | Translation<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/读书-Reading/" title="读书 | Reading">读书 | Reading<sup>7</sup></a></li>
		  
		
		  
			<li><a href="/categories/错误解决与优化-Err-Opt/" title="错误解决与优化 | Err&amp;Opt">错误解决与优化 | Err&amp;Opt<sup>7</sup></a></li>
		  
		
		  
			<li><a href="/categories/随笔-Essays/" title="随笔 | Essays">随笔 | Essays<sup>6</sup></a></li>
		  
		
		</ul>
</div>


  
  <div class="archiveslist">
    <p class="asidetitle"><a href="/archives">归档</a></p>
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">十二月 2016</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">九月 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">八月 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">七月 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">六月 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">五月 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">四月 2016</a><span class="archive-list-count">23</span></li></ul>
  </div>


  
  <div class="tagcloudlist">
    <p class="asidetitle">标签云</p>
    <div class="tagcloudlist clearfix">
       <a href="/tags/CS/" style="font-size: 16.67px;">CS</a> <a href="/tags/Deep-Learning/" style="font-size: 12.22px;">Deep Learning</a> <a href="/tags/Git/" style="font-size: 11.11px;">Git</a> <a href="/tags/Hadoop/" style="font-size: 13.33px;">Hadoop</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/Latex/" style="font-size: 10px;">Latex</a> <a href="/tags/Linux/" style="font-size: 11.11px;">Linux</a> <a href="/tags/Python/" style="font-size: 18.89px;">Python</a> <a href="/tags/Spark/" style="font-size: 10px;">Spark</a> <a href="/tags/Summary/" style="font-size: 14.44px;">Summary</a> <a href="/tags/TensorFlow/" style="font-size: 10px;">TensorFlow</a> <a href="/tags/Web/" style="font-size: 12.22px;">Web</a> <a href="/tags/hexo/" style="font-size: 14.44px;">hexo</a> <a href="/tags/吃喝玩乐/" style="font-size: 10px;">吃喝玩乐</a> <a href="/tags/大数据/" style="font-size: 13.33px;">大数据</a> <a href="/tags/影视/" style="font-size: 11.11px;">影视</a> <a href="/tags/数据分析/" style="font-size: 16.67px;">数据分析</a> <a href="/tags/机器学习/" style="font-size: 20px;">机器学习</a> <a href="/tags/深度学习/" style="font-size: 15.56px;">深度学习</a> <a href="/tags/算法/" style="font-size: 12.22px;">算法</a> <a href="/tags/论文阅读/" style="font-size: 10px;">论文阅读</a> <a href="/tags/译文/" style="font-size: 11.11px;">译文</a> <a href="/tags/读书/" style="font-size: 17.78px;">读书</a> <a href="/tags/随笔/" style="font-size: 14.44px;">随笔</a>
    </div>
  </div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="http://www.cnblogs.com/whatbeg/" target="_blank" title="Old Blog">Old Blog</a>
            
          </li>
        
          <li>
            
            	<a href="https://github.com/whatbeg" target="_blank" title="My Github">My Github</a>
            
          </li>
        
          <li>
            
            	<a href="http://www.matrix67.com/blog/" target="_blank" title="Matrix67">Matrix67</a>
            
          </li>
        
          <li>
            
            	<a href="http://www.liaoxuefeng.com/" target="_blank" title="廖雪峰">廖雪峰</a>
            
          </li>
        
          <li>
            
            	<a href="http://www.ruanyifeng.com/blog/" target="_blank" title="阮一峰">阮一峰</a>
            
          </li>
        
          <li>
            
            	<a href="http://mindhacks.cn/" target="_blank" title="刘未鹏">刘未鹏</a>
            
          </li>
        
          <li>
            
            	<a href="http://machinelearningmastery.com/blog/" target="_blank" title="ML MYSTERY">ML MYSTERY</a>
            
          </li>
        
          <li>
            
            	<a href="http://freemind.pluskid.org/" target="_blank" title="Free Mind">Free Mind</a>
            
          </li>
        
          <li>
            
            	<a href="http://blog.csdn.net/zouxy09/" target="_blank" title="zouxy机器学习">zouxy机器学习</a>
            
          </li>
        
          <li>
            
            	<a href="http://coolshell.cn" target="_blank" title="酷壳">酷壳</a>
            
          </li>
        
    </ul>
</div>

  <table height=30 cellSpacing=0 cellPadding=0 width=180 border=0>
<form action="http://www.sogou.com/web" target="_blank">
<tr style='font-size:12px;color:#000000'>
<td align="center" width=100><input type="text" name="query" size=14 style='BORDER-RIGHT: #999 1px solid; BORDER-TOP: #999 1px solid; BORDER-LEFT: #999	1px	solid; BORDER-BOTTOM: #999 1px solid; HEIGHT: 19px; BACKGROUND-COLOR: #fff'>
<input type="hidden" name="insite" value="whatbeg.com">
<input type="hidden" name="insite2" value="whatbeg.com"></td>
<td align="left" width=45><input type="submit" name="sogou_submit" value="搜索">
</td></tr></form>
</table>



  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

  <div class="rsspart">
	<a href="http://eepurl.com/cHO5An" target="_blank" title="email">Email 订阅</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
        

	    
		
				<div class="cc-license">
          <a href="http://creativecommons.org/licenses/by-nc-nd/4.0" class="cc-opacity" target="_blank">
            <img src="/img/cc-by-nc-nd.svg" alt="Creative Commons" />
          </a>
        </div>
    

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2016-2017 
		
		<a href="/about" target="_blank" title="whatbeg">whatbeg</a>
		
		<br>
		<span class="post-count">Total words: <span style="color:orange">230.2k</span></span>
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
        <span id="busuanzi_container_site_pv">
        &nbsp;&nbsp;Total visits:&nbsp;<span style="color:orange" id="busuanzi_value_site_pv"></span>
        </span>
        <span id="busuanzi_container_site_uv">
        &nbsp;&nbsp;You are Visitor No.<span style="color:orange" id="busuanzi_value_site_uv"></span>
        </span>
        </br>
		</p>
		
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    $('#toc.toc-aside').css('display', 'block').addClass('fadeIn');  //侧边栏显示文章目录
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');   //侧边栏显示widget
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
        getSize();
        if (myWidth >= 1024) {
          c.click();
        }
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>





<script type="text/javascript">

var disqus_shortname = 'whatbeg';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>






<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e6d1f421bbc9962127a50488f9ed37d1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1258390595'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1258390595%26online%3D1%26show%3Dline' type='text/javascript'%3E%3C/script%3E"));</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->
<script>
(function(){
    var bp = document.createElement('script');
    bp.src = '//push.zhanzhang.baidu.com/push.js';
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
  </body>
</html>


